[ { "title": "VCPKG, como usar ?", "url": "/posts/VCPKG_como_usar/", "categories": "c++, programação, vcpkg, compilar, cmake, opencv, windows", "tags": "vcpkg windows c++", "date": "2022-03-05 21:18:34 -0300", "snippet": "Antes de tudo, o que é VCPKG ?Te respondo , é um gerenciador de pacotes tal como Pip (Python) ou Maven (Java) para o C++, que carecia de um gerenciador tão poderoso como esse.Criado pela Microsoft, visa centralizar, gerenciar e utilizar os pacotes com Cmake e seu compilador para ser de fácil utilização seus pelos desenvolvedoresTe levarei por cada passo que é importante fazer para que o VCPKG funcione de forma coerente.Aahh ia me esquecendo, também funciona para o Linux.Começando pelo começo, este tutorial está voltado ao Windows.Primeiro instale o Cmake em sua máquina.Link : https://cmake.org/download/ Agora instale o Visual Studio Community, ele é necessário por conter o compilador de C e C++ necessários para compilar as bibliotecas.Nota : Usaremos a versão comunity em inglês nesse tutorial.Ao instalar o Visual Studio, selecione Desenvolvimento para desktop em C++. Usando o Git Bash para baixar Vcpkg atráves do PowerShell (administrador) iremos começar o procedimento de instalação.git clone https://github.com/Microsoft/vcpkg.gitAcesse a pastacd vcpkg/Digite os comandos abaixo:.\\bootstrap-vcpkg.bat.\\vcpkg.exe integrate installAo final da instalação, irá aparecer o seguinte resultado.-DCMAKE_TOOLCHAIN_FILE=D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmakeEssa informação é importante para quando criar um novo projeto C++ e precisar linkar as bibliotecas, é nesse local que o Cmake irá montar as informações.Mais abaixo irei mostrar como usar essa configuração numa IDE.Adicionando o Auto complete:.\\vcpkg.exe integrate powershellAinda no terminal na pasta do clone do VCPKG, testa o funcionamento instale a lib Curl, que serve para fazer chamadas HTTP..\\vcpkg.exe install curl --debugUsando o toolchain (VCPKG) em uma IDEPara Clion da Jetbrains Para o VSCODE da Microsoft{ &quot;cmake.configureSettings&quot;: { &quot;CMAKE_TOOLCHAIN_FILE&quot;: &quot;D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmake&quot; //Esse caminho se refere ao local onde instalei o VCPKG }, &quot;cmake.configureOnOpen&quot;: true,} Para o QT Creator (Qtzinho da massa para os intímos).Tem uma pequena alteração na string de configuração, mas é simples, só mudar o caminho da pasta que está feito.A configuração passo a passo está no gif meus bacanas.-DCMAKE_TOOLCHAIN_FILE:STRING=D:/andreemidio/libraries/vcpkg/scripts/buildsystems/vcpkg.cmake Código para testar a lib acimaCMakeLists.txtcmake_minimum_required(VERSION 2.8)project(teste)find_package(CURL CONFIG REQUIRED)add_executable(teste main.cpp)target_link_libraries(teste PRIVATE CURL::libcurl)No arquivo de código :main.cpp#include &amp;lt;iostream&amp;gt;#include &amp;lt;curl/curl.h&amp;gt;static size_t WriteCallback(void *contents, size_t size, size_t nmemb, void *userp){ ((std::string*)userp)-&amp;gt;append((char*)contents, size * nmemb); return size * nmemb;}int main(){ CURL * curl; CURLcode res; std::string readBuffer; curl = curl_easy_init(); if(curl) { curl_easy_setopt(curl, CURLOPT_URL, &quot;http://pudim.com.br/&quot;); curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback); curl_easy_setopt(curl, CURLOPT_WRITEDATA, &amp;amp;readBuffer); res = curl_easy_perform(curl); curl_easy_cleanup(curl); std::cout &amp;lt;&amp;lt; readBuffer &amp;lt;&amp;lt; std::endl; } return 0;}O resultado no terminal será algo que está abaixoÉ o nosso saudoso pudim.com.br&amp;lt;html&amp;gt;&amp;lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&amp;gt;&amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Pudim&amp;lt;/title&amp;gt; &amp;lt;link rel=&quot;stylesheet&quot; href=&quot;estilo.css&quot;&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;div&amp;gt; &amp;lt;div class=&quot;container&quot;&amp;gt; &amp;lt;div class=&quot;image&quot;&amp;gt; &amp;lt;img src=&quot;pudim.jpg&quot; alt=&quot;&quot;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&quot;email&quot;&amp;gt; &amp;lt;a href=&quot;mailto:pudim@pudim.com.br&quot;&amp;gt;pudim@pudim.com.br&amp;lt;/a&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;script&amp;gt; (function(i,s,o,g,r,a,m){i[&#39;GoogleAnalyticsObject&#39;]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,&#39;script&#39;,&#39;//www.google-analytics.com/analytics.js&#39;,&#39;ga&#39;); ga(&#39;create&#39;, &#39;UA-28861757-1&#39;, &#39;auto&#39;); ga(&#39;send&#39;, &#39;pageview&#39;);&amp;lt;/script&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;Andre EmidioDesenvolvedor em Visão Computacional/Backend e membro do Grupo OpenCV Brasil." }, { "title": "Aquisição de dados usando o Sensor Kinect do Xbox 360", "url": "/posts/capturando_dados_kinect/", "categories": "kinect, openkinect, c++", "tags": "kinect libfreenec c++", "date": "2022-02-03 21:14:34 -0300", "snippet": "Neste artigo trago um rápido tutorial para que você possa começar a utilizar o sensor kinect do xbox 360 (Kinect v1) para adquirir imagens RGB e mapa de profundidade. Usaremos a libfreenect (OpenKinect) e a OpenCV em linguagem C++ para aprendermos a obter dados do kinect.1. Instalando as bibliotecasCompilando a OpenCV#dependencias da opencvsudo apt-get updatesudo apt-get install build-essential cmake unzip pkg-configsudo apt-get install libjpeg-dev libpng-dev libtiff-devsudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-devsudo apt-get install libxvidcore-dev libx264-devsudo apt-get install libgtk-3-devsudo apt-get install libatlas-base-dev gfortran libeigen3-devcd ~git clone https://github.com/opencv/opencv.gitgit clone https://github.com/opencv/opencv_contrib.gitcd ~/opencvmkdir build &amp;amp;&amp;amp; cd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D INSTALL_PYTHON_EXAMPLES=OFF \\ -D INSTALL_C_EXAMPLES=ON \\ -D OPENCV_ENABLE_NONFREE=ON \\ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules \\ -D BUILD_EXAMPLES=ON ..make -j4sudo make installCompilando a libfreenectcd ~sudo apt-get install git build-essential libusb-1.0-0-dev cython3 libglfw3-dev#instalando a librealsensesudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE || sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDEsudo add-apt-repository &quot;deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main&quot; -usudo apt-get install librealsense2-dkmssudo apt-get install librealsense2-utils# para compilar os exemplossudo apt-get install freeglut3-dev libxmu-dev libxi-devgit clone https://github.com/OpenKinect/libfreenectcd libfreenectmkdir build &amp;amp;&amp;amp; cd buildcmake -DBUILD_PYTHON3=ON -DCYTHON_EXECUTABLE=/usr/bin/cython3 ..#pra caso você queira utilizar também com python#lembre-se que você deve ter o python e a numpy instalados para usar as flags acimamakesudo make installldconfig -v2. Criando o ProjetoPreparando o CMakeListsCrie uma pasta chamada kinect-project em algum lugar de sua preferência. Dentro desta pasta, crie um arquivo chamado main.cpp e um arquivo chamado CMakeLists.txt.A estrutura de seu projeto deve ficar desta forma:--kinect-project |__ main.cpp |__ CMakeLists.txtDentro do arquivo CMakeLists.txt coloque o conteúdo abaixo:cmake_minimum_required(VERSION 2.8 FATAL_ERROR)find_package(OpenCV REQUIRED)find_package(Threads REQUIRED)find_package(libfreenect REQUIRED)include_directories( /usr/local/include/libfreenect/ #verifique se os includes da sua instalação realmente estão neste caminho /usr/local/include/libusb-1.0/ #idem pra esses includes aqui)add_executable( kinect_project main.cpp)target_link_libraries(kinect_project ${OpenCV_LIBS} ${CMAKE_THREAD_LIBS_INIT} ${FREENECT_LIBRARIES} freenect)Escrevendo o CódigoCaso esteja com pressa, o código-fonte completo deste artigo pode ser obtido aqui. (Apesar do módulo se chamar Calibration, o código ainda não faz isso).Inicializando a freenect e acessando o dispositivoA primeira coisa que precisamos fazer é adicionar os includes necessários para o nosso projeto:#include &amp;lt;signal.h&amp;gt;#include &amp;lt;stdbool.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;string.h&amp;gt;#include &quot;libfreenect.h&quot;#include &amp;lt;opencv2/core.hpp&amp;gt;#include &amp;lt;opencv2/highgui.hpp&amp;gt;#include &amp;lt;opencv2/imgproc.hpp&amp;gt;Agora podemos começar a escrever o conteúdo da nossa função main. A primeira coisa a se fazer é inicializar a freenect:int main(int argc, char** argv){ freenect_context* fn_ctx; int ret = freenect_init(&amp;amp;fn_ctx, NULL); if (ret &amp;lt; 0) return ret; //Caso não tenha sucesso em inicializar, para a execução}O próximo passo é encontrar os dispositivos kinect conectados em sua máquina (você conectou o kinect, não é?). Para isso, adicione o seguinte trecho no seu código:int num_devices = ret = freenect_num_devices(fn_ctx);if (ret &amp;lt; 0) return ret;if (num_devices == 0){ std::cout &amp;lt;&amp;lt; &quot;Nenhum dispositivo conectado!&quot; &amp;lt;&amp;lt; std::endl; freenect_shutdown(fn_ctx); return 1;}Depois que encontramos os dispositivos conectados, devemos acessar pelo menos um deles para começarmos a obter os dados. Fazemos isso adicionando o seguinte trecho de código:freenect_device* fn_dev; // Variável que vai guardar as informações do dispositivoret = freenect_open_device(fn_ctx, &amp;amp;fn_dev, 0); //Acessando o primeiro dispositivo que possui indice 0 na listaif (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); // Caso não consiga acessar o dispositivo, encerra a freenect e para a execução return ret;}Agora que já temos acesso ao dispositivo, podemos configurar o modo que receberemos as informações do sensor RGB e do sensor de profundidade:ret = freenect_set_depth_mode(fn_dev, freenect_find_depth_mode(FREENECT_RESOLUTION_MEDIUM, FREENECT_DEPTH_MM));if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}ret = freenect_set_video_mode(fn_dev, freenect_find_video_mode(FREENECT_RESOLUTION_MEDIUM, FREENECT_VIDEO_RGB));if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}Criando as funções de CallbackPara para sermos capazes de manipular os frames que recebemos dos sensores RGB e IR, precisamos definir as funções de callback, que serão chamadas sempre que um novo frame chegar. Então, vamos criar uma função callback para receber o frame RGB e outra callback para receber o frame de profundidade.//função callback para o frame de profundidadevoid depth_cb(freenect_device* dev, void* data, uint32_t timestamp){ printf(&quot;Received depth frame at %d\\n&quot;, timestamp); cv::Mat depthMat(cv::Size(640,480),CV_16UC1); uint16_t* depth = static_cast&amp;lt;uint16_t*&amp;gt;(data); depthMat.data = (uchar*) data; cv::imwrite(&quot;depth.png&quot;, depthMat);}//Função callback para o frame RGBvoid video_cb(freenect_device* dev, void* data, uint32_t timestamp){ printf(&quot;Received video frame at %d\\n&quot;, timestamp); cv::Mat rgbMat(cv::Size(640,480), CV_8UC3, cv::Scalar(0)); uint8_t* rgb = static_cast&amp;lt;uint8_t*&amp;gt;(data); rgbMat.data = rgb; cv::imwrite(&quot;rgb.png&quot;, rgbMat);}Agora, dentro da função main precisamos precisamos “registrar” essas funções callback para que sejam chamadas a cada novo frame:freenect_set_depth_callback(fn_dev, depth_cb); //Lembra que fn_dev é a variável que guarda as informações do dispositivo, né?freenect_set_video_callback(fn_dev, video_cb);Iniciando a captura de framesPara iniciar a captura dos frames, utilizamos a função freenect_start_depth para capturar os frames de profundidade e freenect_start_video para captura dos frames RGB. Copie e cole o trecho abaixo:ret = freenect_start_depth(fn_dev);if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}ret = freenect_start_video(fn_dev);if (ret &amp;lt; 0){ freenect_shutdown(fn_ctx); return ret;}Para manter a captura rodando até que uma interrupção seja requisitada, vamos usar um recurso chamado signals (sinais). A ideia é emitir um sinal toda vez que uma requisição de interrupção do nosso programa for feita. Para isso, vamos declarar uma variável global que armazena o estado do nosso programa e uma função que irá lidar com o sinal (handler). Copie e cole o trecho de código abaixo://variável globalvolatile bool running = true;//função que será chamada quando o sinal de interrupção for emitidovoid signalHandler(int signal){ if (signal == SIGINT || signal == SIGTERM || signal == SIGQUIT) { running = false; }}Agora, dentro da função main colocamos o seguinte loop:while (running &amp;amp;&amp;amp; freenect_process_events(fn_ctx) &amp;gt;= 0){}Depois que o nosso loop for interrompido, devemos parar a execução da freenect de forma apropriada. Então, depois do loop, cole o seguinte trecho de código:std::cout &amp;lt;&amp;lt; &quot;Parando a execução&quot; &amp;lt;&amp;lt; std::endl;// Stop everything and shutdown.freenect_stop_depth(fn_dev);freenect_stop_video(fn_dev);freenect_close_device(fn_dev);freenect_shutdown(fn_ctx);std::cout &amp;lt;&amp;lt; &quot;Encerrado com sucesso!&quot; &amp;lt;&amp;lt; std::endl;return 0;Compilando e Executando o ProjetoAgora podemos finalmente compilar e executar nosso projeto. Para isso, utilizaremos o cmake, então basta executar os seguintes comandos dentro da pasta do nosso projeto:mkdir build &amp;amp;&amp;amp; cd buildcmake ..make./kinect_projectPronto! Agora você verá alguns prints passando no seu terminal (prints que vieram das funções callback) e você pode requisitar a interrupção do programa com um Ctrl+c. Depois que a execução parar, é só olhar dentro da sua pasta build, as imagens rgb.png e depth.png estão salvas lá dentro.Assim você acabou de adquirir uma imagem RGB e uma imagem de profundidade usando o sensor Kinect com a lifreenect e a OpenCV :)Com muito carinho e um pouco de agressão…Natália AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCV Brasil." }, { "title": "Como contribuir neste Blog", "url": "/posts/como_contribuir/", "categories": "contribuicao, artigos", "tags": "getting started", "date": "2022-01-23 16:42:34 -0300", "snippet": "Aprenda como enviar o seu tutorial em forma de artigo para o Blog oficial do Grupo OpenCV Brasil (apelidado carinhosamente de OpenCVismo Brasil).1. Fork o repositório do BlogAcesse o repositório do nosso blog e dê um fork.2. Escreva seu artigo em um arquivo markdownAntes de escrever o conteúdoAntes de começar a escrever o conteúdo propriamente dito, lembre-se de colocar o cabeçalho que todos os nossos artigos devem ter, um exemplo:---title: O título do seu Artigoauthor: name: Seu Nome link: link para seu github ou linkedindate: 2022-01-23 16:42:34 -0300categories: [categaria 1, categoria2]tags: [tag1 tag2]pin: false--- Escreva seu título entre aspas duplas (sim, é uma string); Escreva seu nome e coloque um link para uma rede social sua: Pode ser seu blog, linkedin ou github. O importante é o leitor te encontrar! Modifique a data para a data em que você escreveu seu artigo, o formato da data é aaaa-mm-dd (ano-mês-dia); Escreva em quais categorias seu conteúdo se encaixa (Ex: filtros deepLearning Yolo). As categorias devem ser separadas por espaço e cada artigo deve ter no máximo três categorias. O mesmo vale para as tags Inserindo ImagensImagens podem ser inseridas no artigo para melhorar o entendimento do leitor. Você pode utilizar a própria sintaxe da linguagem markdown para fazer isso de forma simples e rápida. Um exmeplo:![image info](link-para-a-sua-imagem)Se você produziu imagens para inserir no seu tutorial, crie uma pasta dentro de /assets/img/imagens/nome-da-sua-pasta e coloque as imagens que você produziu dentro do seu diretório. Assim, você pode chamar essas imagens em seu artigo da seguinte forma:![image info]({{ site.baseurl }}/assets/img/imagens/nome-da-sua-pasta/sua-imagem.png)Inserindo código-fonteEste não deveria precisar de maiores explicações, não é? Para inserir seu código fonte, basta usar a sintaxe padrão do markdown para código: Conteúdo do código entre uma par de três crases. Um exemplo:Deixe sua assinatura no artigoNo final do artigo você pode colocar uma assinatura para deixar as pessoas saberem que você é o autor. Olhe como os outros membros fizeram suas assinaturas e construa a sua.Salvando o artigoSeu artigo deve estar em um arquivo markdown (.md) e deve ser colocado dentro do diretório _posts. Fique atento para dar o nome correto ao seu arquivo, perceba que o nome do arquivo .md tem uma padronização, por exemplo:2022-01-23-como_contribuir.md Primeiro insira a data que você criou este conteúdo no formato AAAA-MM-DD (ano-mês-dia); Depois coloque um título resumido do seu artigo (com no máximo três palavras separadas por underline); O nome do seu artigo deve ficar com o mesmo padrão do exemplo acima. 3. Deploy localVocê pode testar o blog localmente em sua máquina, assim você testa se o site está funcionando com seu artigo antes de enviar a pull request e quebrar o site blog. As instruções sobre o que você precisa instalar e como rodar subir o site localmente podem ser encontradas aqui . Não seja preguiçoso(a), leia! :)4. Envie uma Pull RequestAgora que você já escreveu, se atentou para os padrões necessários, conferiu se o site não quebrou com as suas atualizações, é hora de enviar uma PR para qe seu artigo entre para o blog oficial: Abra uma PR e marque Natália Carvalho (NataliaCarvalho03) como revisora; Espere que o processo de revisão termine, caso haja alguma coisa a corrigir, você será avisado. ConclusãoEntão, garotos e garotas, é isso! Sintam-se a vontade para contribuir com novos artigos e qualquer dúvida, basta nos contatar nos grupos do Telegram ou Discord!Com muito carinho e um pouco de agressão…Natália C. de AmorimEngenheira em Visão Computacional e fundadora do Grupo OpenCV Brasil" }, { "title": "Detecção de objetos com haarcascade", "url": "/posts/cap5/", "categories": "haarcascade, detecção, classificador", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de objetos usando o método Haar CascadeNesse capítulo você irá aprender uma maneira rápida e direta de como criar um classificador Haar Cascade. Além disso, no final do capítulo será disponibilizado um código para detecção de objetos com o classificador criado, que com pequenas alterações, pode se adequar a qualquer situação.Nesse contexto, o método Haar Cascade, é um método de detecção de objetos proposto por Paul Viola e Michael Jones. É uma abordagem baseada em Machine Learning, em que uma função cascade é treinada com muitas imagens positivas e negativas. Logo, é usado para detectar objetos em outras imagens.Preparando o ambientePara esse projeto é necessário que você tenha instalado em sua máquina apenas 3 itens.1 - Editor de textos de sua preferência, eu particularmente uso o Visual Studio Code.2 - Alguma versão Python de sua preferência, eu particularmente uso a 3.8.5.3 - Biblioteca OpenCV .Aqui não irei explicar como você pode fazer o download e instalação desses itens, pois na internet existem diversos tutorias detalhados de como fazer isso.PassosA criação de um classficador usando o HaarCascade pode ser descrita em um conjunto de 5 passos.1 - Escolher o objeto.2 - Selecionar imagens negativas.3 - Selecionar imagens positivas.4 - Gerar o vetor de positivas.5 - Treinar o classificador.1 - Escolher o objeto.O primeiro passo é escolher o objeto que será identificado, para isso você deverá pensar nos seguintes aspectos:* Serão objetos rígidos como uma logo (nike) ou com variações (cadeira,copo)?* Objetos rigidos são mais eficientes e mais rápidos.* Ao treinar muitas variações pode ser que o classificador fique fraco, portanto, fique atento a isso.* Objetos que a cor é fundamental não são recomendados, pois as imagens serão passadas para a escala de cinza.Para esse projeto, escolhi o objeto faca para ser detectado.2 - Selecionar imagens negativas.Para selecionar as imagens negativas, você deve ficar atento aos seguintes aspectos:* Podem ser qualquer coisa, menos o objeto.* Devem ser maiores que as positivas, pois a openCV vai colocar as imagens positivas dentro das negativas.* Se possível usar fotos de prováveis fundos onde o objeto é encontrado. *Ex: Objeto = carro Usar imagens de asfalto e ruas vazias.Logo, você deve ficar atento as imagens escolhidas, pois como dito elas podem ser qualquer coisa, exceto o objeto escolhido, como escolhemos facas como objeto de detecção devemos, iremos precisar de imagens que não tenham facas.Quantas imagens negativas?É relativo, para esse projeto eu conto com 3000 imagens negativas, com diversas variações de fundo. Entretanto, isso vai depender dos resultados obtidos no treinamento, pode ser que eu precise de mais imagens ou não, isso será explicado mais a frente com mais detalhes.Exemplos de imagens negativas: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x100, essa informação será importante para futuras explicações.Aqui é importante mencionar que você deve criar uma pasta (ex: projeto) onde estará outra pasta com as imagens negativas, na pasta projeto também deve estar as imagens positivas.Exemplo: PastaNa pasta das imagens negativas você deve colocar esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/criar_lista.batE deve executá-lo ao final da escolha das imagens negativas, pois ele vai gerar uma lista com as imagens negativas.3 -Selecionar imagens positivas.Para selecionar as imagens positivas, você deve ficar atento aos seguintes aspectos:* Apenas o objeto.* Quantas imagens? * Depende da: Qualidade da imagem, tipo do objeto, poder computacional disponível.* As imagens devem ter o mesmo tamanho e a proporção precisa ser a mesma, caso contrário a openCV faz isso automaticamente e gera problemas de distorção do objeto. * Ex: Uma imagem 100x50, passada pra 25x25, vai ter o objeto descaracterizado.* Imagens grandes podem gerar problemas, fazendo o treinamento durar até meses.* Sempre que possível usar imagens com fundo branco.Como dito no primeiro passo, você deve tomar cuidado com as variações do objeto, caso você queira realizar a detecção de um objeto em diferentes ângulos é sugerido que você faça diferentes classificadores. Para exemplificar isso, cito o classificador haarcascade frontal face, que realiza a detecção frontal da face (esse classificador inclusive é de fácil acesso, até a openCV disponibiliza ele pra você) e caso você queira a detecção lateral da face, você deve usar outro classificador, esses cuidados devem ser tomados para que você tenha um bom classificador.Em relação a quantidade, alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 mil imagens como entrada para o treinamento.Exemplos de imagens positivas que irei usar para o treinamento do classificador: Figura 1 Figura 2 Figura 3OBS: Todas essas imagens tem dimensões 100x50, essa informação será importante para explicações futuras.Aqui temos o pulo do gato, é possível criar mais imagens positivas a partir das imagens que você já tem, para isso baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_createsamples.exeColoque ele junto com as imagens positivas, depois basta abrir o CMD, entrar na pasta onde estão suas imagens positivas e digitar esse comando:opencv_createsamples -img faca_1.png -bg negativas/bg.txt -info positivas/positivas.lst -maxxangle 0.5 -maxyangle 0.5 -maxzangle 0.5 -num 300 -bgcolor 255 -bgthresh 10Parâmetros:-img = Nome da imagem base.-bg = Nome da pasta / nome do arquivo .txt com as informações das imagens negativas.-info = Nome da pasta / Nome do arquivo .lst (sempre altere esse parâmetro quando usar uma nova imagem (Ex: positivas2/positivas2.lst, positivas3/positivas3.lst)).-maxangle (x,y,z) = Variação de rotação que a imagem terá.-num = Número de imagens que serão criadas.-bgtresh = parâmetro que permite a retirada do fundo da imagem, deixando apenas o objeto de interesse (aqui se justifica o fundo branco). Esse parâmetro deve ser analisado com cuidado, pois: bgtresh 10 bgtresh 100Como resultado você terá a quantidade de imagens mencionada em uma pasta de acordo com o nome que você escolheu e um arquivo .lst que terá informações sobre essas imagens, esse arquivo é de extrema importância e é a partir dele que iremos criar o vetor de imagens.Nesse caso, usei 10 imagens e criei um total de 3000 imagens a partir desse comando, logo, você terá as pastas positivas1, positivas2 e etc…Ex: Pasta4 - Gerar o vetor de positivas.Aqui você deverá criar um vetor para cada pasta com as imagens positiva (positivas1, positivas2, etc…) e depois juntar esses vetores em apenas um vetor.Para isso, digite esse comando no CMD:opencv_createsamples -info positivas1/positivas1.lst -num 2000 -w 50 -h 25 -vec vetor1.vecParâmetros:-info = Nome da pasta que contém as imagens / arquivo .lst (Como criamos 3000 imagens a partir de 10 imagens, temos 10 pastas com 300 imagens cada, logo, teremos que repetir esse comando 10 vezes, alterando o nome da pasta e do arquivo .lst, para positivas2 / positivas2.lst, etc…). w e -h = são as dimensões, como nossas imagens eram 100 x 50, eu reduzi para 50 x 25, para reduzir o tamanho do arquivo, até porque para treinar o classificador com as imagens em 100 x 50 eu deveria ter um super computador. vec = Nome do vetor (Aqui você também tem que alterar, colocando vetor1, vetor2, etc…). Após isso, devemos unir todos esses vetores em apenas um, para isso crie uma pasta chamada “vec” e coloque todos os vetores nela.Depois baixe esse arquivo:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/mergevec.pye coloque ele na pasta do seu projeto.após isso, digite no CMD:python mergevec.py -v vec/ -o vetor_final.vecApós a conclusão você terá um arquivo chamado vetor_final.vec, que é o vetor que iremos utilizar.Nesse momento a pasta do seu projeto estará assim: Pasta “Projeto”5 - Treinar o classificadorPara o passo final, você deve primeiramente baixar esses arquivos:https://github.com/luis131313/cookbook/blob/master/imagens/cap2/opencv_traincascade.exehttps://mega.nz/file/09YnVKQb#LdE1iz05i9OeoMqoZtuC3lVn4teeA7gqozVS-N1hG2UApós isso, você deve abrir a pasta negativas e colocar esses dois arquivos e o vetor final nela, e criar uma pasta chamada “classificador”.Dessa maneira: Pasta “classificador” ArquivosApós isso, abra o CMD na pasta negativas e digite o seguinte comando:opencv_traincascade -data classificador -vec vetor_final.vec -bg bg.txt -numPos 2400 -numNeg 1200 -numStages 15 -w 30 -h 15 -precalcBufSize 1024 -precalcIdxBufSize 1024Parâmetros-data = Nome da pasta que os arquivos de treinamento serão armazenados.-vec = Nome do vetor.-bg = informações das imagens negativas.-numPos = Número de imagens positivas.-numNeg = Número de imagens negativas.-numStages = Número de estágios.-w e -h = dimensões das imagens.-precalcBufSize e -precalcIdxBufSize = memória utilizada para o treinamento.Após o treinamento, na pasta classificador você terá esses arquivos: ArquivosO arquivo cascade.xml é o nosso classificador.O arquivo params.xml são os parâmetros usados no treinamento.E os outros arquivos, são os resultados de cada estágio do treinamento.Sobre o uso dos parâmetros: É indicado que você use metade do número de imagens positivas para as negativas no primeiro treinamento. Alguns estudos sugerem que um bom classificador deve ter no mínimo 5000 imagens positivas. Após o primeiro treinamento, se você notar que está tendo muitos falsos positivos, aumente o número de imagens negativas, se notar que não está realizando a detecção, aumente o número de imagens positivas, faça novos treinamentos até ter bons resultados. Para melhorar os resultados você também pode aumentar o número de estágios. Não se esqueça que a soma dos parâmetros -precalcBufSize e -precalcIdxBufSize não pode ser maior que a memória disponível. Quanto mais imagens negativas, positivas, estágios de treinamento e dimensão das imagens, mais o treinamento vai demorar, podendo fazer o treinamento durar até meses. Código para detecçãoAgora irei apresentar um código que irá realizar a detecção de facas.Para isso baixe esse classificador que eu criei, ele ainda não está pronto, logo não irá apresentar resultados excelentes.https://github.com/luis131313/cookbook/blob/master/imagens/cap2/cascade_facas.xmlimport cv2#variável que armazena a imagemimagem1 = &#39;teste1.png&#39;#variável que armazena o arquivo xmlcascade_path1 = &#39;cascade_facas.xml&#39; #cria o classificadorclf1 = cv2.CascadeClassifier(cascade_path1)#lê a imagemimg1 = cv2.imread(imagem1)#converte para cinzagray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)#Função da detecçãodeteccoes1 = clf1.detectMultiScale(gray1, scaleFactor=1.01, minNeighbors=5, minSize=(1,1))#desenha o retângulo com as coordenadas obtidasfor(x,y,w,h) in deteccoes1: img1 = cv2.rectangle(img1, (x,y), (x+w, y+h), (0,0,255), 2)#para visualizar a imagemcv2.imshow(&#39;Classificador 1&#39;, img1)#mantém a janela aberta até que eu digite uma teclacv2.waitKey(0)#destrói a janelacv2.destroyAllWindows()Ao executar o código com um exemplo, teremos essa detecção: Imagem retirada do Google Podemos notar alguns falsos positivos, o que indica que seria interessante realizar um novo treinamento com mais imagens negativas.Considerações finaisVários dos arquivos apresentados foram cedidos pela www.iaexpert.academy, agradeço imensamente pela generosidade.Fiz esse tutorial com muito carinho e espero que seja útil para você, a intenção aqui foi realizar um pequeno projeto usando o método HaarCascade, ainda existe muito há aprender sobre esse método, mas a minha intenção é apenas introduzir esse assunto.Desejo bons estudos e bons trabalhos.Atenciosamente,Luis Fernando Santos Ferreira, Aluno do curso de Ciência da Computação na Universidade Federal de Lavras.Linkedin: https://www.linkedin.com/in/luis-ferreira-3b02131a8/" }, { "title": "Filtro de densidade usando Machine Learning e Clusterização", "url": "/posts/cap3/", "categories": "agrupamento, densidade, dbscan", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Filtro de densidade usando Machine Learning e ClusterizaçãoNesse capítulo você irá aprender uma maneira rápida e fácil de utilizar Machine learning para remover ruídos em uma captura de objeto utilizando range de cores.Na captura de imagens utilizando cores, um dos grandes problemas encontrados é a remoção do ruído indesejado por objetos menores ao fundo do cenário.Nesse contexto, iremos utilizar o método DBSCAN da Lib SKLEARN para reorganizar os pixels das cores selecionadas e identificar áreas de baixa densidade.Obs. Os códigos contidos neste capítulo foram desenvolvidos para serem rodados no Google Colab e pode sofrer algumas alterações para rodar fora do Colab (exemplo: a utilização da lib google.colab.patches para visualizar imagens).Preparando o ambientePara esse projeto é necessário que você tenha instalado os seguintes itens em sua máquina. Editor de textos de sua preferência. Python 3.8.5 . Bibliotecas (OpenCV, Collections, Sklearn, Numpy, urllib, matplotlib). Caso queira, pode utilizar o Google Colab que tem o ambiente praticamente pronto.PassosImportar Libsimport numpy as npimport urllibimport cv2from google.colab.patches import cv2_imshowfrom collections import Counterfrom sklearn.cluster import DBSCANfrom sklearn import metricsfrom sklearn.datasets import make_blobsfrom sklearn.preprocessing import StandardScalerimport matplotlib.pyplot as pltDownload da imagemBaixamos a imagem diretamente da internet utilizando a lib Urlib e após isso, utilizamos as libs Numpy e Opencv para converter a imagem para um formato aceito pela lib OpenCV.Obs. Como estamos utilizando o Google Colab, estaremos realizando o download e conversão das imagens diretamente do Imgur.def url_to_image(url): resp = urllib.request.urlopen(url) image = np.asarray(bytearray(resp.read()), dtype=&quot;uint8&quot;) image = cv2.imdecode(image, cv2.IMREAD_COLOR) hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) return hsvurl = f&#39;https://i.imgur.com/PLGlnWj.png&#39;img = url_to_image(url)Seleção do range de corEste é um dos momentos mais demorados e manuais da aplicação, pois teremos que manualmente encontrar o range de cores utilizados no objeto que queremos selecionar.No nosso caso é o Azul.img = url_to_image(url)BLUE_MIN = (110,50,50)BLUE_MAX = (130,255,255)Separação de pixelsNesta parte do código percorremos todos os pixels da imagem, verificamos quais encontram-se dentro do range de cores selecionados e guardamos as coordenadas dos azuis dentro do array data_corddata_cord = []height, width, channels = img.shapefor x in range(height): for y in range(width): r, g, b = img[x,y] if (r,g,b) &amp;gt;= BLUE_MIN and (r,g,b) &amp;lt;= BLUE_MAX: data_cord.append([int(x),int(y)])DBSCANAgora, com o data_cord já separado, iremos utilizar DBSCAN para clusterizar os dados.O DBSCAN é uma algoritmo de machine learning que clusteriza os dados com base em densidade e tamanho de cluster.X = StandardScaler().fit_transform(data_cord)db = DBSCAN(eps=0.1, min_samples=1).fit(X)core_samples_mask = np.zeros_like(db.labels_, dtype=bool)core_samples_mask[db.core_sample_indices_] = Truelabels = db.labels_Apresentação de clusters (Opcional)Este tópico não é obrigatório, mas caso queira visualizar os dados e verificar se está sendo separado corretamente.Utilizando Matplotlib, separamos os labels dos clusters e setamos uma cor para cada cluster.n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)unique_labels = set(labels)colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]for k, col in zip(unique_labels, colors): if k == -1: pass class_member_mask = (labels == k) xy = X[class_member_mask &amp;amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=14) xy = X[class_member_mask &amp;amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=tuple(col), markeredgecolor=&#39;k&#39;, markersize=6)plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_)plt.show()Identificar maior clusterUtilizando o código abaixo, separamos os clusters em dict, verificamos qual o maior e salvamos essa informação em template_max.dict_data = dict(Counter(labels))template_max = max(dict_data, key=dict_data.get)ApresentaçãoNo código abaixo, realizamos a varredura dentro de labels e para cada vez que identificamos o maior cluster, pegamos este index e buscamos a mesma posição no data_cord.Assim, conseguimos pegar as coordenadas que se encontram no maior cluster e podemos manipular as coordenadas do maior cluster, ignorando o ruído geral da imagem.No caso abaixo, estamos pintando de vermelho o maior cluster e pintando de azul os demais(ruídos) apenas para fins demonstrativos.for i in range(len(labels)): if labels[i] == template_max: x,y = data_cord[i] img[x,y] = (0,0,255) else: x,y = data_cord[i] img[x,y] = (255,0,0)cv2_imshow(img)Atenciosamente&amp;lt;/br&amp;gt;Willian Jesus da Silva, Aluno do curso de Ciência da Computação no Instituto de ensino superior da Grande Florianópolis." }, { "title": "Processamento Morfológico de Imagens", "url": "/posts/cap2/", "categories": "Erosão, Dilatação, Morfologia", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Processamento Morfológico de ImagensImagens e ConjuntosEm algumas aplicações de Processamento Digital de Imagens podemos utilizar conceitos da Teoria dos conjuntos para realizarmos algumas análises e inferir certas informações em imagens. Por exemplo, podemos dizer que um objeto, representado em uma imagem, é formado pelo conjunto de pixels que o constituem. Figura 1: Pista de Corrida Na Figura 1 você pode perceber facilmente algumas árvores e uma pista de corrida. Podemos dizer que somos capazes de determinar visualmente as diferenças entre as árvores e a pista de corrida apenas afirmando que o conjunto dos pixels azuis formam a pista de corrida, enquanto o conjunto dos pixels de cor verde-escura formam as árvores.O que eu quero te mostrar neste capítulo é que podemos realizar algumas operações sobre estes conjuntos de pixels que representam objetos nas imagens! A estas operações sobre conjuntos de pixels chamamos de Processamento Morfológico de Imagens.Os Elementos EstruturantesJá sabemos que podemos considerar objetos representados em imagens como conjuntos e que podemos também realizar operações sobre estes conjuntos. Bom, mas se vamos fazer operações sobre este conjunto de pixels, precisamos de um outro conjunto para realizar estas operações, não é mesmo? Muito bem! Para realizarmos estas operações precisamos dos Elementos estruturantes!Observe a Figura 2, na qual temos uma imagem representando um objeto em tons de cinza que é imóvel, enquanto que, se movendo, temos um pequeno conjunto de pixels que está percorrendo esta imagem (algo parecido com um filtro). Figura 2: Erosão A Figura 2 mostra uma operação entre dois conjuntos: O elemento estruturante (filtro que se move ao longo da imagem) e o objeto representado na imagem. O resultado desta operação é uma redução da quantidade de pixels que presentam o objeto. Esta operação é chamada de Erosão.Como funciona a Erosão em uma imagem?Vamos considerar que o conjunto A é o objeto representado na imagem e que o conjunto B é o elemento estruturante, ambos apresentados na Figura 2. Primeiro é feita uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto caso todos os elementos de B que são diferentes de zero estejam contidos em A, caso contrário, descarta-se este pixel. (No exemplo da imagem, a cor cinza representa o valor 1 e a cor branca representa o valor zero). Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto menor de pixels conforme mostra a Figura 2. Outra Operação: A Dilatação Figura 3: Dilatação Observe na Figura 3 o procedimento que está sendo representado: Temos o conjunto A (o objeto representado na imagem), o conjunto B (elemento estruturante) e novamente realizamos uma varredura de B em A. A diferença é que agora, ao contrário do processo de Erosão, o objeto representado na imagem não “perdeu” pixels, isto é, ao invés de diminuirmos o conjunto A, ele ficou ainda maior. Quando isto acontece dizemos que o conjunto A sofreu Dilatação.O que aconteceu neste caso é que modificamos a operação a ser realizada entre estes dois conjuntos para aplicar a Dilatação, assim podemos dizer que a dilatação é aplicada seguindo os seguintes passos: Faz-se uma “varredura” do conjunto B (Elemento estruturante) em A para que a origem de B passe por todos os elementos de A. Depois é feita uma verificação: Para cada localização da origem de B, considera-se que o pixel de A é um membro do novo conjunto se pelo menos um elemento de B esteja contido em A, caso contrário, descarta-se este pixel. Perceba que antes mesmo que a origem do elemento estruturante esteja contida dentro do conjunto A, o pixel abaixo dela já está contida dentro de A, portanto considera-se que a posição da origem do elemento estruturante é parte do novo conjunto A. Depois de fazer essa varedura em toda a imagem, o resultado final é alcançado com o objeto tendo um conjunto maior de pixels conforme mostra a Figura 3. Mão na massa com a OpenCV e PythonPré-requisitosO que será necessário para realizar os tutoriais a seguir: Python 3.x instalado em sua máquina; OpenCV 4; Um editor de código ou IDE de sua preferência (Eu utilizo o VS Code); Os dados que utilizaremos para executar os tutoriais podem ser baixados aqui.Exemplo 1: ErosãoObserve a imagem abaixo. Note que não há muitos detalhes nesta imagem e isso pode facilitar as coisas para nós. Figura 4: Exemplo 1 Primeiro vamos supor que, por algum motivo, queremos eliminar a linha que conecta os circulos. Esta é uma operação de remoção de alguns pixels, por isso usaremos a Erosão para remover os pixels desta linha.Com a OpenCV o processo de aplicar a erosão é muito simples:import cv2, sysimport numpy as np#lendo a imagemimg = cv2.imread(&quot;ex1.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()#criando um elemento estruturante de tamanho 5x5kernel = np.ones((5,5),np.uint8)#Aplicando a erosãoerosao = cv2.erode(img, kernel, iterations = 1)cv2.imwrite(&quot;erosao_ex1.jpg&quot;, erosao)No código acima utilizamos um elemento estruturante (que também pode ser chamado de kernel) de tamanho 5x5 para aplicar a erosão. Neste kernel, todos os valores são iguais a 1. Você pode criar kernels personalizados que tenham também valores igual a zero, mas aqui utilizamos desta forma para facilitar.O parâmetro iterations é o número de vezes que a erosão será aplicada à imagem. No nosso caso, aplicamos a erosão apenas uma vez.E temos o seguinte resultado: Figura 5: Resultado da Erosão da Fig. 4 Sinta-se à vontade para brincar um pouco com os parâmetros deste trecho de código! Veja o que acontece quando o kernel tem tamanhos menores, tamanhos maiores e quando o número de iterações é maior!Podemos dizer que as principais aplicações para a erosão são: Remoção de ruídos na imagem; Remoção de atributos que não são interessantes para a aplicação em questão. Podemos imaginar que os círculos brancos são topos de postes e que a linha era um fio passando por eles. Através da erosão removemos o fio. Exemplo 2: DilataçãoVamos supor agora que temos uma imagem na qual os círculos tem alguns “buracos” e queremos fechá-los. Precisamos de uma operação que possa adiiconar pixels aos circulos brancos, logo, precisamos da Dilatação. Figura 6: Exemplo 2 E novamente podemos usar a openCV para implementar facilmente esta operação:import cv2, sysimport numpy as npimg = cv2.imread(&quot;ex2.png&quot;,0)if img is None: print(&quot;Não foi possível ler a imagem!&quot;) sys.exit()kernel = np.ones((5,5),np.uint8)dilatacao = cv2.dilate(img,kernel,iterations = 2)cv2.imwrite(&quot;dilatacao_ex2.jpg&quot;, dilatacao)Neste exemplo usamos o mesmo kernel que utilizamos no exemplo anterior e aplicamos a dilatação duas vezes (iterations = 2) para fechar os círculos brancos.Temos o seguinte resultado: Figura 7: Resultado da Dilatação da Fig. 6Te convido novamente a realizar testes alterando o tamanho do kernel e a quantidade de iterações para ver o que acontece em cada caso!Quero novamente chamar a sua atenção para o fato de que nós não apenas “fechamos os círculos”, mas note que os círculos estão maiores do que na imagem original (Fig. 4) e estão assumindo uma forma um pouco retangular, por isso devemos usar a dilatação com cuidado para que não modifiquemos demais as características de um objeto na imagem!Podemos dizer que as principais aplicações da Dilatação são: Quando queremos que objetos sejam “destacados” fazendo com que eles fiquem maiores na imagem; Quando queremos “fechar buracos” ou até mesmo conectar elementos que estão muito próximos um do outro na imagem, porém não o suficiente para se conectarem. AtenciosamenteNatália C. de AmorimMestre em Ciências Geodésicas e Doutoranda em Ciências Geodésicas na Universidade Federal do Paraná." }, { "title": "Detecção de Bordas", "url": "/posts/cap1/", "categories": "canny, bordas", "tags": "", "date": "2020-12-04 08:25:34 -0300", "snippet": "Detecção de BordasA visão computacional é uma área da ciência que desenvolve teorias e tecnologias como objetivos de extrair informações de dados multidimensionais. Quase sempre, recorremos a uma analogia de como nós detectamos e reconhecemos objetos. Um objeto é caracterizado por conjuntos de atributos como: cor, texturas e forma geométrica. Nesse sentido, a extração de contorno poder representar informações importantes sobre um determinado objeto. Por exemplo, podemos identificar diversas formas geométricas como retângulo circulo, triângulos, linhas e outros. Além do que os médoto de detecção não utilizam muito recurso computacional sendo uma técnica atraente para aplicação em sistemas embaracados.Nessa capítulo vamos conhecer a base dos algoritmos de detecção de borda e aplicar o algoritmo de Canny.DependênciasPara executar os scripts mostrado aqui, você precisará ter em sua máquina uma versão do python 3 e o OpenCV instalados. python3 OpenCVO que é uma borda?Uma borda á caracteriza por uma variação abrupta entre os pixels vizinhos de uma imagem. Figura 1: Pista de corrida Primeiro vamos analizar apenas na linha selecionada Figura 1, podemos representa-la por uma função I(x) cujo domínio é uma lista [254,254,173,138,79,44,45,53]Como nossa função é discreta (só admite valor inteiro) não podemos calcular diretamente a derivada dessa função mais podemos fazer uma boa aproximação.A derivada é uma operação matemática que permite calcular a taxa de variação de uma função ou de dois pontos muito próximos. Ela é definida pela equação 1. Equção 1: Derivada. Supondo que x seja a posição que estamos na lista, então f(x) é o valor do pixel e f(x+h) é próximo pixel. Acontece que, quando o intervalo h for muito pequeno vamos pegar variações decorreste de ruídos na imagem. sendo assim, não vamos preocupar em fazer pequenos ajustes nesse sentido. Por exemplo, podemo dizer que nossa derivada no ponto x é dada por f(x+h)-f(x-h), ou seja, a diferença do próximo pixel pelo pixel anterior ao ponto x. Equação 2: Derivada aproximada. Desconsideramos a divisão por h da Equação 1, porque nesse contexto ele é apenas um normalizador da função, ou seja, ele será um parâmetro que vamos passar ao realizar os cálculos. &amp;lt;Figura 2: Derivada aproximada para 1. Um dos motivos da aproximação de fizemos é por conta dos ruídos, porém essa nova equação pode ser representada por um kernel. Computacionalmente é mais interessante convolver um kernel por uma imagem do que aplicar uma função. Figura 3: Kernel para calculo de derivada. Na Figua 4 realizamos essa operação para toda a linha da imagem 1, tente identificar onde está a região que selecionamos. Figura 4: Grafico de linha da selecionada na figura 1. A lista começa com valor alto, 254 decai ate 44 e sobe novamente para 53. Essa variação acontece no intervalo 160 à 178 (aproximado) do eixo x.Se expandirmos esse ideia para um plano 2D nossa função anterior pode ser descrita da seguinte forma. Figura 5: Aproximação de derivada. Agora temos derivadas parciais. Da mesma forma, podemos rescrever isso por um kernel. Figura 6: Kernel para derivada parcial. Esse par de kernel na Figura 6 tem o nome de operador Sobel. O OpenCV tem esse operador implementado aqui cv2.sobel, então vamos usar.import cv2ddepth = cv2.CV_16S# Carrega imagen &quot;frame.png&quot; em escala de cinzagray = cv2.imread(&quot;frame.png&quot;,cv2.IMREAD_GRAYSCALE)# calcula derivada de primeira ordem na direção x grad_x = cv2.Sobel(gray, ddepth, 1, 0, ksize=3,scale = 1)# calcula derivada de primieira ordem na direção ygrad_y = cv2.Sobel(gray, ddepth, 0, 1, ksize=3,scale = 1)# calcula valor absoluto e converte para uint8 abs_grad_x = cv2.convertScaleAbs(grad_x)abs_grad_y = cv2.convertScaleAbs(grad_y)# Calcula gradientegrad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)# concatena image gerada com a originalsaida=cv2.hconcat((grad,gray))# redimenciona em 60%saida=cv2.resize(saida,None,None,0.4,0.4)#salva imagemcv2.imwrite(&quot;saida.png&quot;,saida)cv2.imshow(&quot;janela&quot;, saida)cv2.waitKey(0) Figura 7: Resultado do Sobel.Perceba que aplicamos o operador Sobel duas vezes, primeiro na direção x e depois na direção y. A composição dessas derivadas é matematicamente conhecida como gradiente. O gradiente é um vetor que aponta na direção onde a função tem a maior variação. No entanto, o que nos interessa aqui é magnitude desse gradiente, ou seja, o quão abrupta é essa variação. O módulo do gradiente poder ser calulado usando a Equação 2 (calculamos com a função cv2.addWeighted). Equação 3: Magnitude do gradiente. O Sobel é uma das operações mais relevantes para detectar contorno em imagens. Embora exista alternativas como cv2.Scharr que tem uma aproximação melhor da derivada. O Sobel ainda é um dos principais métodos empregados nos algoritmos para detecção de borda.Algoritmo de CannyO algoritmo de Canny executa vários estágio para detectar uma borda.1. remoção de ruídos.Na Figura 4, o gráfico da derivada apresenta bastante ruido, isso acontece porque pegamos micros variações locais. Canny usa um filtro gaussiano para resolver isso. Veja como o filtro afeta a derivada na Figura 8. Figura 8: Efeito de filtro gaussiano.2. Calcular gradientes.O filtro Sobel discutido no tópico anterior é usado aqui para calcular os gradientes.3. Máximos locais.Nessa etapa uma varredura completa é realizada na imagem em busca de gradientes máximos locais, esse processo elemina bordas largas ou duplicadas.4. Limiar de hesterese.Tudo que esta abaixo de minVal é descartado. o que esta entre minVal e maxVal é mantido apenas se parte do contorno estiver acima de maxVal. Na Figura 9, A é mantido porque esta acima de maxVal, C é mantido, embora esteja abaixo de maxVal ele esta conectado a A. Já o B é removido, pois esta totalmente dentro da área delimitada. Figura 9: Região delimitada pelos liminar maxVal e minVal.Fonte: https://docs.opencv.org/master/da/d22/tutorial_py_canny.htmlUsando CannyNo OpenCV temos uma implementação do algoritmo de Canny, o segundo e o terceiro parâmetros passados, são minVal e maxVal.import cv2# Carrega imagem em escala de cinzagray = cv2.imread(&#39;frame.png&#39;,cv2.IMREAD_GRAYSCALE)# aplica algoritmo de Canny com minVal=100 e maxVal=200edges = cv2.Canny(gray,100,200)# concatena imagem original com o resultadosaida=cv2.hconcat((gray,edges))#redimenciona saida=cv2.resize(saida,None,None,0.4,0.4)#Mostra saidacv2.imshow(&quot;janela&quot;,saida)cv2.waitKey() Figura 9: Resultado do Canny. Aqui deixo um vídeo com animação gráfica do que discutimos nesse artigo.ConclusãoDe fato, a área de visão computacional é permeada por aplicações matemáticas de alta complexidade. No entanto, bibliotecas como OpenCV tem simplificado, permitindo que pessoas de diversas áreas desenvolva suas própias aplicações. Se você gostou desse assunto, junte-se a nós no grupo opencvBrasil .AtenciosamenteElton fernandes dos SantosEngenheiro eletricista pela UNEMAT e mestrando em Zootecnia na Universidade Federal do Mato Grosso UFMT.Autor do blog visioncompyReferências Documentação oficial OpenCV v 4.5.0: Sobel. fonte https://docs.opencv.org/master/d2/d2c/tutorial_sobel_derivatives.html Documentação oficial OpenCV v 4.5.0: Algoritmo de Canny. fonte https://docs.opencv.org/master/da/d22/tutorial_py_canny.html " } ]
